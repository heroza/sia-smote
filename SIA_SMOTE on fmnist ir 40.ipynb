{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/sia-smote/blob/main/SIA_SMOTE%20on%20fmnist%20ir%2040.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library"
      ],
      "metadata": {
        "id": "LsLmMJXd10KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_W = 28\n",
        "IMAGE_H = 28\n",
        "IMAGE_C = 1\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "INPUT_SHAPE = (IMAGE_H, IMAGE_W,)\n",
        "num_classes = 2"
      ],
      "metadata": {
        "id": "xavhBtEH24Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "# %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "import random\n",
        "\n",
        "from typing_extensions import Counter\n",
        "from sklearn.utils import shuffle\n",
        "import heapq\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score, fbeta_score\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iCH5udu13Sr",
        "outputId": "b133c9ff-52c4-4528-a490-72cbba1c0d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_fmnist_im():\n",
        "  (X_train, y_train), (X_val, y_val) = fashion_mnist.load_data()\n",
        "\n",
        "  # To generate the indices of the data that we want. (Train)\n",
        "  idx_train = np.concatenate(\n",
        "      (\n",
        "      np.where(y_train == 0)[0],\n",
        "      np.random.choice(np.where(y_train == 1)[0], 150, replace=False)\n",
        "      #  np.where(y_train == 2)[0][:4800],\n",
        "      #  np.where(y_train == 3)[0][:4200],\n",
        "      #  np.where(y_train == 4)[0][:3600],\n",
        "      #  np.where(y_train == 5)[0][:3000],\n",
        "      #  np.where(y_train == 6)[0][:2400],\n",
        "      #  np.where(y_train == 7)[0][:1800],\n",
        "      #  np.where(y_train == 8)[0][:1200],\n",
        "      #  np.where(y_train == 9)[0][:600]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  X_train = X_train[idx_train]\n",
        "  y_train = y_train[idx_train]\n",
        "  #y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "\n",
        "  X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "\n",
        "  # To generate the indices of the data that we want. (val)\n",
        "  idx_val = np.concatenate(\n",
        "      (\n",
        "      np.where(y_val == 0)[0],\n",
        "      np.where(y_val == 1)[0][:25]\n",
        "      #  np.where(y_val == 2)[0][:800],\n",
        "      #  np.where(y_val == 3)[0][:700],\n",
        "      #  np.where(y_val == 4)[0][:600],\n",
        "      #  np.where(y_val == 5)[0][:500],\n",
        "      #  np.where(y_val == 6)[0][:400],\n",
        "      #  np.where(y_val == 7)[0][:300],\n",
        "      #  np.where(y_val == 8)[0][:200],\n",
        "      #  np.where(y_val == 9)[0][:100]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  X_val = X_val[idx_val]\n",
        "  y_val = y_val[idx_val]\n",
        "  #y_val = to_categorical(y_val, num_classes=num_classes)\n",
        "\n",
        "  X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
        "  return (X_train, y_train), (X_val, y_val)\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def join_data(X_train,y_train,resx1,resy1):\n",
        "  X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  # y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, IMAGE_W, IMAGE_H, IMAGE_C)\n",
        "  return X_train, y_train"
      ],
      "metadata": {
        "id": "nNuY0prK_E8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading"
      ],
      "metadata": {
        "id": "XxPQ_9gQ1r80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkc-4ZTY0-F-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418e3ec2-b7c0-4c3c-b901-5cfe03cdd2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load fmnist imbalance\n",
        "(X_train, y_train), (X_val, y_val) = make_fmnist_im()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize\n",
        "# prepare train and test sets\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "\n",
        "# normalize values\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0"
      ],
      "metadata": {
        "id": "O1yEIkjv2fK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SIAMESE Training"
      ],
      "metadata": {
        "id": "jqIcF66Y1-Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pairs(x, digit_indices):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n",
        "    \n",
        "    for d in range(num_classes):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, num_classes)\n",
        "            dn = (d + inc) % num_classes\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "            \n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def create_pairs_on_set(images, labels):\n",
        "    \n",
        "    digit_indices = [np.where(labels == i)[0] for i in range(num_classes)]\n",
        "    pairs, y = create_pairs(images, digit_indices)\n",
        "    y = y.astype('float32')\n",
        "    \n",
        "    return pairs, y\n",
        "\n",
        "\n",
        "def show_image(image):\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.colorbar()\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2lSIH3Kl2sB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create pairs on train and test sets\n",
        "tr_pairs, tr_y = create_pairs_on_set(X_train, y_train)\n",
        "ts_pairs, ts_y = create_pairs_on_set(X_val, y_val)\n",
        "print(Counter(tr_y))\n",
        "print(Counter(ts_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YyargYT2rFi",
        "outputId": "18fd03f3-b08d-4cfc-c64e-cf27d09a2709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1.0: 298, 0.0: 298})\n",
            "Counter({1.0: 48, 0.0: 48})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_base_network():\n",
        "    input_shape = (IMAGE_H, IMAGE_W,)\n",
        "    input = Input(shape=input_shape, name=\"base_input\")\n",
        "    x = Flatten(name=\"flatten_input\")(input)\n",
        "    x = Dense(128, activation='relu', name=\"first_base_dense\")(x)\n",
        "    x = Dropout(0.1, name=\"first_dropout\")(x)\n",
        "    x = Dense(128, activation='relu', name=\"second_base_dense\")(x)\n",
        "    x = Dropout(0.1, name=\"second_dropout\")(x)\n",
        "    x = Dense(128, activation='relu', name=\"third_base_dense\")(x)\n",
        "\n",
        "    return Model(inputs=input, outputs=x)\n",
        "\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "def contrastive_loss_with_margin(margin):\n",
        "    def contrastive_loss(y_true, y_pred):\n",
        "        '''Contrastive loss from Hadsell-et-al.'06\n",
        "        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "        '''\n",
        "        square_pred = K.square(y_pred)\n",
        "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "        return (y_true * square_pred + (1 - y_true) * margin_square)\n",
        "    return contrastive_loss\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    pred = y_pred.ravel() < 0.5\n",
        "    return np.mean(pred == y_true)"
      ],
      "metadata": {
        "id": "pvHDHQ3Z18QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_network = initialize_base_network()\n",
        "#plot_model(base_network, show_shapes=True, show_layer_names=True, to_file='base-model.png')"
      ],
      "metadata": {
        "id": "ieE9OzL83sMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the left input and point to the base network\n",
        "input_shape = (IMAGE_H, IMAGE_W,)\n",
        "input_a = Input(shape=input_shape, name=\"left_input\")\n",
        "vect_output_a = base_network(input_a)\n",
        "\n",
        "# create the right input and point to the base network\n",
        "input_b = Input(shape=input_shape, name=\"right_input\")\n",
        "vect_output_b = base_network(input_b)\n",
        "\n",
        "# measure the similarity of the two vector outputs\n",
        "output = Lambda(euclidean_distance, name=\"output_layer\", output_shape=eucl_dist_output_shape)([vect_output_a, vect_output_b])\n",
        "\n",
        "# specify the inputs and output of the model\n",
        "model = Model([input_a, input_b], output)\n",
        "\n",
        "# plot model graph\n",
        "#plot_model(model, show_shapes=True, show_layer_names=True, to_file='outer-model.png')"
      ],
      "metadata": {
        "id": "uB9kcy6X3xre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rms = RMSprop()\n",
        "model.compile(loss=contrastive_loss_with_margin(margin=1), optimizer=rms)\n",
        "history = model.fit([tr_pairs[:,0], tr_pairs[:,1]], tr_y, epochs=20, batch_size=128, validation_data=([ts_pairs[:,0], ts_pairs[:,1]], ts_y))"
      ],
      "metadata": {
        "id": "pM25_3lY3ztm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2524106-a499-4062-843d-ffac79491824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 5s 68ms/step - loss: 0.6970 - val_loss: 0.1830\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.2035 - val_loss: 0.1069\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1652 - val_loss: 0.0849\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.1237 - val_loss: 0.0627\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.1193 - val_loss: 0.0480\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0876 - val_loss: 0.0400\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0778 - val_loss: 0.0676\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0599 - val_loss: 0.0324\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0450 - val_loss: 0.0293\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0566 - val_loss: 0.0405\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0351 - val_loss: 0.0215\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0263 - val_loss: 0.0210\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0318 - val_loss: 0.0295\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0443 - val_loss: 0.0604\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0368 - val_loss: 0.0220\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0233 - val_loss: 0.0205\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.0163 - val_loss: 0.0200\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.0188 - val_loss: 0.0259\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.0229 - val_loss: 0.0149\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.0164 - val_loss: 0.0115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = model.evaluate(x=[ts_pairs[:,0],ts_pairs[:,1]], y=ts_y)\n",
        "\n",
        "y_pred_train = model.predict([tr_pairs[:,0], tr_pairs[:,1]])\n",
        "train_accuracy = compute_accuracy(tr_y, y_pred_train)\n",
        "\n",
        "y_pred_test = model.predict([ts_pairs[:,0], ts_pairs[:,1]])\n",
        "test_accuracy = compute_accuracy(ts_y, y_pred_test)\n",
        "\n",
        "print(\"Loss = {}, Train Accuracy = {} Test Accuracy = {}\".format(loss, train_accuracy, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16KxVAXi34H9",
        "outputId": "888123bd-24e3-4d5e-84e3-ac7923c1cbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step - loss: 0.0115\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 4ms/step\n",
            "Loss = 0.011534787714481354, Train Accuracy = 1.0 Test Accuracy = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SIA-SMOTE Oversampling"
      ],
      "metadata": {
        "id": "FccRzOQ55qTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Euclidean_Metric(a,b):\n",
        "      dis = np.linalg.norm(a - b)\n",
        "      return dis\n",
        "      \n",
        "def G_SM(All_X,samples_Y, n_to_sample, cl):\n",
        "    g_index=0\n",
        "    \n",
        "    Minority_X=All_X[samples_Y == 1] # 1 is Minority class\n",
        "    print(f'all x len: {All_X.shape}, Minority_X shape: {Minority_X.shape}')\n",
        "    #Populate distance matrix\n",
        "    dis_matrix=np.zeros((Minority_X.shape[0],All_X.shape[0]),dtype=float)\n",
        "    for i in range(0,Minority_X.shape[0]):\n",
        "        for j in range(0,All_X.shape[0]):\n",
        "            dis_matrix[i,j]=Euclidean_Metric(Minority_X[i],All_X[j])\n",
        "            if(dis_matrix[i,j]==0):\n",
        "                dis_matrix[i,j]=999999\n",
        "    dis_matrix=dis_matrix.tolist()\n",
        "    print('here')\n",
        "    #noise filtering\n",
        "    base_indices=[] # d = noise, minority class which its nearest neihbor is majority class\n",
        "    neighbor_indices=[]\n",
        "    #print(Minority_X.shape[0])\n",
        "    for i in range(Minority_X.shape[0]):\n",
        "        min_index=list(map(dis_matrix[i].index, heapq.nsmallest(1, dis_matrix[i])))\n",
        "        #print(min_index)\n",
        "        if(samples_Y[min_index[0]]==0): \n",
        "            base_indices.append(i)\n",
        "            neighbor_indices.append(min_index[0])\n",
        "    # Minority_X=np.delete(Minority_X,d,axis=0)\n",
        "    print('base_indices len',len(base_indices))\n",
        "    # dis_matrix = np.array(dis_matrix)\n",
        "    \n",
        "    # base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    base_indices = np.random.choice(base_indices,n_to_sample)\n",
        "    neighbor_indices = np.random.choice(neighbor_indices,n_to_sample)\n",
        "\n",
        "    X_base = Minority_X[base_indices]\n",
        "    X_neighbor = All_X[neighbor_indices]\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1), X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return X_base, samples, [cl]*n_to_sample\n",
        "\n",
        "def SIAMESE_SMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #oversampling\n",
        "  # resx = []\n",
        "  # resy = []\n",
        "\n",
        "  # for i in range(num_classes):\n",
        "  #xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "  # n = np.max(counter) - counter[i]\n",
        "  # if n <= 0:\n",
        "  #   continue\n",
        "  xbase, xsamp, ysamp = G_SM(X_train,y_train,5400,1)\n",
        "  ysamp = np.array(ysamp)\n",
        "  # resx.append(xsamp)\n",
        "  # resy.append(ysamp)\n",
        "  \n",
        "  # resx1 = np.vstack(resx)\n",
        "  # resy1 = np.hstack(resy)\n",
        "\n",
        "  #selection of new samples based on Siamese Network\n",
        "  # y_pred = model.predict(xbase, xsamp)\n",
        "  # pred = y_pred.ravel() < 0.5\n",
        "\n",
        "  # resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  # X_train = np.vstack((resx1,X_train))\n",
        "  # y_train = np.hstack((resy1,y_train))\n",
        "  # y_train = to_categorical(y_train)\n",
        "  # X_train = X_train.reshape(-1, IMAGE_W, IMAGE_H)#,3) #BEWARE IMAGE CHANNEL\n",
        "  # return X_train, y_train\n",
        "  return xbase, xsamp, ysamp"
      ],
      "metadata": {
        "id": "DCOSo3sR5I7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbase, xsamp, ysamp = SIAMESE_SMOTE_Data(X_train, y_train, one_hot = False)\n",
        "print(f'xbase shape {xbase.shape}, xsamp shape {xsamp.shape}, ysamp shape {ysamp.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbDiUagX5uzI",
        "outputId": "10ab80d8-a458-4560-c08f-a8d0a1ec5df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all x len: (6150, 784), Minority_X shape: (150, 784)\n",
            "here\n",
            "base_indices len 12\n",
            "xbase shape (5400, 784), xsamp shape (5400, 784), ysamp shape (5400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbase = xbase.reshape(xbase.shape[0],IMAGE_W,IMAGE_H)\n",
        "xsamp = xsamp.reshape(xsamp.shape[0],IMAGE_W,IMAGE_H)\n",
        "print(f'xbase shape {xbase.shape}, xsamp shape {xsamp.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7akkfGz511g",
        "outputId": "c3863c7b-1afc-4ebe-d901-289784a7ab2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xbase shape (5400, 28, 28), xsamp shape (5400, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#selection of new samples based on Siamese Network\n",
        "y_pred = model.predict([xbase, xsamp])\n",
        "pred = y_pred.ravel() < 0.03\n",
        "xsamp_sia = xsamp[pred]\n",
        "ysamp_sia = ysamp[pred]\n",
        "print(f'xsamp_sia shape {xsamp_sia.shape}, ysamp_sia shape {ysamp_sia.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpIb-QTb59eB",
        "outputId": "3ce1ca29-4ed6-495d-80c8-f97c662bd9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169/169 [==============================] - 0s 2ms/step\n",
            "xsamp_sia shape (160, 28, 28), ysamp_sia shape (160,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SMOTE"
      ],
      "metadata": {
        "id": "-xdGhiOJNdKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "    \n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1), X_neighbor - X_base)\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def SMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  \n",
        "  #oversampling\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n <= 0:\n",
        "        continue\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "      resx.append(xsamp)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  #X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  # y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, IMAGE_W, IMAGE_H, 1)\n",
        "  return X_train, y_train"
      ],
      "metadata": {
        "id": "Z5nrRwDoNfBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def G_SM2(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "    \n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1), X_neighbor - X_base)\n",
        "    return X_base, samples, [cl]*n_to_sample\n",
        "\n",
        "def SMOTE_Data2(X_train, y_train, one_hot = False):\n",
        "  X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  \n",
        "  #oversampling\n",
        "  resx = []\n",
        "  basex = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n <= 0:\n",
        "        continue\n",
        "      xbase, xsamp, ysamp = G_SM2(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "      resx.append(xsamp)\n",
        "      basex.append(xbase)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  xsamp = np.vstack(resx)\n",
        "  xbase = np.vstack(basex)\n",
        "  resy1 = np.hstack(resy)\n",
        "\n",
        "  xbase = xbase.reshape(xbase.shape[0],IMAGE_W,IMAGE_H)\n",
        "  xsamp = xsamp.reshape(xsamp.shape[0],IMAGE_W,IMAGE_H)\n",
        "  print(f'xbase shape {xbase.shape}, xsamp shape {xsamp.shape}')\n",
        "\n",
        "  #selection of new samples based on Siamese Network\n",
        "  y_pred = model.predict([xbase, xsamp])\n",
        "  pred = y_pred.ravel() < 0.1\n",
        "  resx1 = xsamp[pred]\n",
        "  resy1 = resy1[pred]\n",
        "  print('shape after selection with siamese',resx1.shape)\n",
        "\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  # y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, IMAGE_W, IMAGE_H, 1)\n",
        "  return X_train, y_train"
      ],
      "metadata": {
        "id": "_yZ6fakZQrZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ASN-SMOTE"
      ],
      "metadata": {
        "id": "GNuVMvLoaulI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Apr 11 17:25:10 2021\n",
        "\n",
        "@author: 易新凯\n",
        "\"\"\"\n",
        "#NM-Smote\n",
        "\n",
        "#分类器\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "#标准化工具\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "#导入集合分割，交叉验证，网格搜索\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_validate,KFold,StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#smote过采样\n",
        "# from imblearn.over_sampling import SMOTE,BorderlineSMOTE,SVMSMOTE,ADASYN,KMeansSMOTE,RandomOverSampler\n",
        "# #欠采样\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "import time\n",
        "def generate_x(samples,N,k):\n",
        "    #n=int(N/10)\n",
        "    time_start=time.time()\n",
        "    g_index=0\n",
        "    wrg=0\n",
        "    samples_X=samples.iloc[:,0:-1]\n",
        "    samples_Y=samples.iloc[:,-1]\n",
        "    Minority_sample=samples[samples.iloc[:,-1].isin([1])] # 1 is Minority class\n",
        "    Minority_sample_X=Minority_sample.iloc[:,0:-1]\n",
        "                                       \n",
        "    # transfer = StandardScaler()\n",
        "    # SMinority_X= transfer.fit_transform(Minority_sample)\n",
        "    # All_X=transfer.fit_transform(samples_X)\n",
        "    Minority_X=np.array(Minority_sample_X)\n",
        "    All_X=np.array(samples_X)\n",
        "    n1=All_X.shape[0]-2*Minority_X.shape[0]\n",
        "    print(n1)\n",
        "    #n=int((All_X.shape[0]-2*Minority_X.shape[0])/Minority_X.shape[0])\n",
        "    #print(n)\n",
        "\n",
        "    #Populate distance matrix\n",
        "    dis_matrix=np.zeros((Minority_X.shape[0],All_X.shape[0]),dtype=float)\n",
        "    for i in range(0,Minority_X.shape[0]):\n",
        "        for j in range(0,All_X.shape[0]):\n",
        "            dis_matrix[i,j]=Euclidean_Metric(Minority_X[i,:],All_X[j,:])\n",
        "            if(dis_matrix[i,j]==0):\n",
        "                dis_matrix[i,j]=999999\n",
        "    dis_matrix=dis_matrix.tolist()\n",
        "    \n",
        "    #noise filtering\n",
        "    d=[] # d = noise, minority class which its nearest neihbor is majority class\n",
        "    #print(Minority_X.shape[0])\n",
        "    for i in range(Minority_X.shape[0]):\n",
        "        min_index=list(map(dis_matrix[i].index, heapq.nsmallest(1, dis_matrix[i])))\n",
        "        #print(min_index)\n",
        "        if(samples_Y[min_index[0]]==0): \n",
        "            d.append(i)\n",
        "    Minority_X=np.delete(Minority_X,d,axis=0)\n",
        "    #print(Minority_X.shape)\n",
        "\n",
        "    n=int((n1)/Minority_X.shape[0])\n",
        "    #print(n)\n",
        "    synthetic = np.zeros(((Minority_X.shape[0])*n,Minority_X.shape[1]),dtype=float)\n",
        "    #print(Minority_X.shape[0])\n",
        "    for i in range(Minority_X.shape[0]):\n",
        "\n",
        "        # Filter in only neihgbours within safe radius\n",
        "        min_index=list(map(dis_matrix[i].index, heapq.nsmallest(k, dis_matrix[i])))\n",
        "        best_index={}\n",
        "        best_f=0\n",
        "        for h in range(len(min_index)):\n",
        "            \n",
        "            if(samples_Y[min_index[h]]==0): # 0 is Majority class\n",
        "               best_index[best_f]=min_index[h]\n",
        "               best_f+=1\n",
        "               break # safe radius has been reached\n",
        "            else:\n",
        "                best_index[best_f]=min_index[h]\n",
        "                best_f+=1\n",
        "        #print(best_index)\n",
        "\n",
        "        # syntesize samples by interpolating base samples and safe neihgbours\n",
        "        for j in range(0,n):\n",
        "            nn=random.randint(0,len(best_index)-1)\n",
        "            #print(min_index[nn])\n",
        "            dif=All_X[best_index[nn]]-Minority_X[i]\n",
        "            #print(dif)\n",
        "            gap=random.random()\n",
        "            synthetic[g_index]=Minority_X[i]+gap*dif\n",
        "            g_index+=1\n",
        "            \n",
        "    #print(synthetic.shape)\n",
        "    #print(wrg)\n",
        "    \n",
        "    # synthetic=synthetic[0:synthetic.shape[0]-,:]\n",
        "    labels=np.ones(synthetic.shape[0])\n",
        "    synthetic=np.insert(synthetic,synthetic.shape[1],values=labels,axis=1)\n",
        "    examples=np.concatenate((samples,synthetic),axis=0)\n",
        "    time_end=time.time()\n",
        "    del(dis_matrix)\n",
        "    return examples\n",
        "def RandomforClassifier(xtrain,ytrain,xtest,ytest):\n",
        "    transfer = StandardScaler()\n",
        "    xtrain = transfer.fit_transform(xtrain)\n",
        "    xtest = transfer.transform(xtest)\n",
        "    #选用随机森林模型\n",
        "    rfc=RandomForestClassifier(\n",
        "                                criterion='gini',\n",
        "                                n_estimators=100,\n",
        "                                min_samples_split=2,\n",
        "                                min_samples_leaf=2,\n",
        "                                max_depth=15,\n",
        "                                random_state=6)\n",
        "    #score_pre = cross_val_score(rfc,xtrain,ytrain,scoring='roc_auc',cv=10).mean()\n",
        "    #scores = cross_val_score(rfc,xtrain,ytrain,cv=10,scoring='roc_auc')\n",
        "    #print(scores)\n",
        "    #print('mean CV-Scores: %.6f' % score_pre)\n",
        "    rfc=rfc.fit(xtrain,ytrain)\n",
        "    # #测试评估\n",
        "    #result=rfc.score(xtest,ytest)\n",
        "    AUC=roc_auc_score(ytest,rfc.predict_proba(xtest)[:,1])\n",
        "    cm=confusion_matrix(ytest,rfc.predict(xtest))\n",
        "    TN=cm[0][0]\n",
        "    FP=cm[0][1]\n",
        "    FN=cm[1][0]\n",
        "    TP=cm[1][1]\n",
        "    Acc=(TP+TN)/(TP+TN+FP+FN)\n",
        "    Pos_Precision=TP/(TP+FP)\n",
        "    #print(\"%.3f\" %(Pos_Precision))\n",
        "    #Neg_Precision=TN/(TN+FN)\n",
        "    Sensitivity=TP/(TP+FN)\n",
        "    Specificity=TN/(TN+FP)\n",
        "    F_Measure=2*Sensitivity*Pos_Precision/(Sensitivity+Pos_Precision)\n",
        "    G_Mean=np.sqrt(Sensitivity*Specificity)\n",
        "    #print(\"F_Measure=%.6f\" % F_Measure)\n",
        "    #print(\"G_Mean=%.6f\" %G_Mean)\n",
        "    #print(\"AUC=%.6f\" %AUC)\n",
        "    #print(\"Acc=%.6f\" % Acc)\n",
        "    bal_acc = balanced_accuracy_score(ytest,rfc.predict(xtest))\n",
        "    return F_Measure,G_Mean,AUC,Acc,bal_acc"
      ],
      "metadata": {
        "id": "UeuGQr9naxh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification"
      ],
      "metadata": {
        "id": "YX8tRwt-6cPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model with padded convolutions for the fashion mnist dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = make_fmnist_im()\n",
        "\t# reshape dataset to have a single channel\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "\t# one hot encode target values\n",
        "\t# trainY = to_categorical(trainY)\n",
        "\t# testY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "# define cnn model\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\topt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# evaluate a model using k-fold cross-validation\n",
        "def evaluate_model(dataX, dataY, xsamp_sia,ysamp_sia, n_folds=10, k_n=5):\n",
        "\tdataX = dataX.reshape((dataX.shape[0], 28, 28, 1))\n",
        "\tconfusion = np.array([[0, 0], [0, 0]])\n",
        "\tscores, histories = list(), list()\n",
        "\tAcc_arr, bal_acc_arr, G_Mean_arr, F_Measure_arr, Precision_arr, Sensitivity_arr, Specificity_arr = list(), list(), list(), list(), list(), list(), list()\n",
        "\t# prepare cross validation\n",
        "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
        "\t# enumerate splits\n",
        "\tfor train_ix, test_ix in kfold.split(dataX):\n",
        "\t\t# define model\n",
        "\t\tmodel = define_model()\n",
        "\t\t# select rows for train and test\n",
        "\t\tX_train, y_train, X_val, y_val = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
        "\t\t#oversampling\n",
        "\t\t# sm = SMOTE(k_neighbors=k_n)\n",
        "\t\t# X_train, y_train = sm.fit_resample(X_train.reshape(X_train.shape[0], IMAGE_W * IMAGE_H), y_train)\n",
        "\t\t# X_train = X_train.reshape(X_train.shape[0], IMAGE_W, IMAGE_H, 1)\n",
        "\t\t#X_train, y_train = SMOTE_Data2(X_train, y_train)\n",
        "\t\t# X_train,y_train = join_data(X_train,y_train,xsamp_sia,ysamp_sia)\n",
        "\t\t#ASN-SMOTE\n",
        "\t\tkdata=pd.DataFrame(np.column_stack((X_train.reshape(X_train.shape[0], IMAGE_W * IMAGE_H),y_train)))\n",
        "\t\tg_sample=generate_x(kdata,100,k_n)\n",
        "\t\tX_train, y_train = g_sample[:,0:-1], g_sample[:,-1]\n",
        "\t\tX_train = X_train.reshape(X_train.shape[0], IMAGE_W, IMAGE_H, 1)\n",
        "\n",
        "\t\t# fit model\n",
        "\t\thistory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\t\t#predict\n",
        "\t\ty_train_pred = model.predict(X_train)\n",
        "\t\ty_train_pred = np.where(y_train_pred < 0.5, 0, 1)\n",
        "\t\ty_val_pred = model.predict(X_val)\n",
        "\t\ty_val_pred = np.where(y_val_pred < 0.5, 0, 1)\n",
        "\t\t\n",
        "\t\t# evaluate model\n",
        "\t\tbal_acc = balanced_accuracy_score(y_val, y_val_pred)\n",
        "\t\tTN, FP, FN, TP = confusion_matrix(y_val, y_val_pred).ravel()\n",
        "\t\tprint('TN, FP, FN, TP: ',TN, FP, FN, TP)\n",
        "\t\t# TN=cm[0][0]\n",
        "\t\t# FP=cm[0][1]\n",
        "\t\t# FN=cm[1][0]\n",
        "\t\t# TP=cm[1][1]\n",
        "\t\tAcc=(TP+TN)/(TP+TN+FP+FN)\n",
        "\t\tPrecision=TP/(TP+FP)\n",
        "\t\t#print(\"%.3f\" %(Pos_Precision))\n",
        "\t\t#Neg_Precision=TN/(TN+FN)\n",
        "\t\tSensitivity=TP/(TP+FN)\n",
        "\t\tSpecificity=TN/(TN+FP)\n",
        "\t\tF_Measure=2*Sensitivity*Precision/(Sensitivity+Precision)\n",
        "\t\tG_Mean=np.sqrt(Sensitivity*Specificity)\n",
        "\t\t# append scores\n",
        "\t\tconfusion += confusion_matrix(y_val, y_val_pred)\n",
        "\t\tAcc_arr.append(Acc)\n",
        "\t\tbal_acc_arr.append(bal_acc)\n",
        "\t\tG_Mean_arr.append(G_Mean)\n",
        "\t\tF_Measure_arr.append(F_Measure)\n",
        "\t\tPrecision_arr.append(Precision)\n",
        "\t\tSensitivity_arr.append(Sensitivity)\n",
        "\t\tSpecificity_arr.append(Specificity)\n",
        "\n",
        "\t\tscores.append(bal_acc)\n",
        "\t\thistories.append(history)\n",
        "\tprint('Acc_arr: mean=%.3f std=%.3f' % (mean(Acc_arr)*100, std(Acc_arr)*100))\n",
        "\tprint('bal_acc_arr: mean=%.3f std=%.3f' % (mean(bal_acc_arr)*100, std(bal_acc_arr)*100))\n",
        "\tprint('G_Mean_arr: mean=%.3f std=%.3f' % (mean(G_Mean_arr)*100, std(G_Mean_arr)*100))\n",
        "\tprint('F_Measure_arr: mean=%.3f std=%.3f' % (mean(F_Measure_arr)*100, std(F_Measure_arr)*100))\n",
        "\tprint('Precision_arr: mean=%.3f std=%.3f' % (mean(Precision_arr)*100, std(Precision_arr)*100))\n",
        "\tprint('Sensitivity_arr: mean=%.3f std=%.3f' % (mean(Sensitivity_arr)*100, std(Sensitivity_arr)*100))\n",
        "\tprint('Specificity_arr: mean=%.3f std=%.3f' % (mean(Specificity_arr)*100, std(Specificity_arr)*100))\n",
        "\n",
        "\treturn scores, histories, confusion\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(histories):\n",
        "\tfor i in range(len(histories)):\n",
        "\t\t# plot loss\n",
        "\t\tpyplot.subplot(211)\n",
        "\t\tpyplot.title('Cross Entropy Loss')\n",
        "\t\tpyplot.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "\t\tpyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
        "\t\t# plot accuracy\n",
        "\t\tpyplot.subplot(212)\n",
        "\t\tpyplot.title('Classification Accuracy')\n",
        "\t\tpyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "\t\tpyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
        "\tpyplot.show()\n",
        "\n",
        "# summarize model performance\n",
        "def summarize_performance(scores):\n",
        "\t# print summary\n",
        "\tprint(scores)\n",
        "\tprint('Accuracy: mean=%.3f std=%.3f' % (mean(scores)*100, std(scores)*100))\n",
        "\t# box and whisker plots of results\n",
        "\tpyplot.boxplot(scores)\n",
        "\tpyplot.show()\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "# def run_test_harness(xsamp_sia,ysamp_sia):\n",
        "\t# load dataset\n",
        "# trainX, trainY, testX, testY = load_dataset()\n",
        "# prepare pixel data\n",
        "# trainX, testX = prep_pixels(X_train, X_val)\n",
        "# evaluate model\n",
        "k_values = [1, 2, 3, 4, 5, 6, 7]\n",
        "for k in k_values:\n",
        "\tscores, histories, confusion = evaluate_model(X_train, y_train, xsamp_sia,ysamp_sia,k_n=k)\n",
        "# # learning curves\n",
        "# summarize_diagnostics(histories)\n",
        "# # summarize estimated performance\n",
        "# summarize_performance(scores) \n",
        "# print(confusion)\n",
        "\n",
        "# entry point, run the test harness\n",
        "# run_test_harness(xsamp_sia,ysamp_sia)"
      ],
      "metadata": {
        "id": "Vx8QZyzN6D5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8126cd-c7a4-4edc-cdc5-4027f1e6cc7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 3ms/step\n",
            "TN, FP, FN, TP:  598 0 0 17\n",
            "5263\n",
            "338/338 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  600 1 1 13\n",
            "5269\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 3 14\n",
            "5265\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 2 1 14\n",
            "5271\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 1 0 18\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  604 0 0 11\n",
            "5267\n",
            "337/337 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 1 0 16\n",
            "5263\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  600 1 1 13\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  603 1 0 11\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 2 2 15\n",
            "Acc_arr: mean=99.707 std=0.228\n",
            "bal_acc_arr: mean=97.398 std=2.954\n",
            "G_Mean_arr: mean=97.321 std=3.070\n",
            "F_Measure_arr: mean=94.169 std=4.310\n",
            "Precision_arr: mean=93.530 std=3.929\n",
            "Sensitivity_arr: mean=94.964 std=5.854\n",
            "Specificity_arr: mean=99.833 std=0.106\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 0 17\n",
            "5263\n",
            "338/338 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 3ms/step\n",
            "TN, FP, FN, TP:  600 1 1 13\n",
            "5269\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 4 13\n",
            "5265\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 3 1 14\n",
            "5271\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  590 7 0 18\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  601 3 0 11\n",
            "5267\n",
            "337/337 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 1 0 16\n",
            "5263\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  600 1 0 14\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  602 2 0 11\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  590 8 2 15\n",
            "Acc_arr: mean=99.415 std=0.461\n",
            "bal_acc_arr: mean=97.311 std=3.710\n",
            "G_Mean_arr: mean=97.207 std=3.921\n",
            "F_Measure_arr: mean=89.328 std=6.791\n",
            "Precision_arr: mean=85.037 std=9.869\n",
            "Sensitivity_arr: mean=95.090 std=7.392\n",
            "Specificity_arr: mean=99.533 std=0.416\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 0 17\n",
            "5263\n",
            "338/338 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  599 2 1 13\n",
            "5269\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 2 2 15\n",
            "5265\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 3ms/step\n",
            "TN, FP, FN, TP:  598 2 1 14\n",
            "5271\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 1 0 18\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 6 0 11\n",
            "5267\n",
            "337/337 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  599 0 0 16\n",
            "5263\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  601 0 1 13\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  602 2 0 11\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 2 2 15\n",
            "Acc_arr: mean=99.593 std=0.284\n",
            "bal_acc_arr: mean=97.626 std=2.369\n",
            "G_Mean_arr: mean=97.573 std=2.432\n",
            "F_Measure_arr: mean=91.742 std=5.945\n",
            "Precision_arr: mean=88.914 std=9.606\n",
            "Sensitivity_arr: mean=95.552 std=4.748\n",
            "Specificity_arr: mean=99.701 std=0.265\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 3ms/step\n",
            "TN, FP, FN, TP:  596 2 0 17\n",
            "5263\n",
            "338/338 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  601 0 1 13\n",
            "5269\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 3 14\n",
            "5265\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 3 1 14\n",
            "5271\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 1 0 18\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  602 2 0 11\n",
            "5267\n",
            "337/337 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  599 0 0 16\n",
            "5263\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 3ms/step\n",
            "TN, FP, FN, TP:  599 2 0 14\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  603 1 0 11\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 3 14\n",
            "Acc_arr: mean=99.659 std=0.224\n",
            "bal_acc_arr: mean=97.437 std=3.445\n",
            "G_Mean_arr: mean=97.341 std=3.603\n",
            "F_Measure_arr: mean=93.119 std=4.242\n",
            "Precision_arr: mean=91.701 std=5.590\n",
            "Sensitivity_arr: mean=95.090 std=6.908\n",
            "Specificity_arr: mean=99.783 std=0.150\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 1 16\n",
            "5263\n",
            "338/338 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 3 1 13\n",
            "5269\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 2 4 13\n",
            "5265\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 2 1 14\n",
            "5271\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  595 2 0 18\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  602 2 0 11\n",
            "5267\n",
            "337/337 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 4ms/step\n",
            "TN, FP, FN, TP:  596 3 0 16\n",
            "5263\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  599 2 0 14\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  602 2 0 11\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  595 3 2 15\n",
            "Acc_arr: mean=99.496 std=0.224\n",
            "bal_acc_arr: mean=97.067 std=3.608\n",
            "G_Mean_arr: mean=96.959 std=3.817\n",
            "F_Measure_arr: mean=90.090 std=4.052\n",
            "Precision_arr: mean=86.381 std=3.494\n",
            "Sensitivity_arr: mean=94.501 std=7.209\n",
            "Specificity_arr: mean=99.633 std=0.100\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 2 0 17\n",
            "5263\n",
            "338/338 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  600 1 1 13\n",
            "5269\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 3 14\n",
            "5265\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 3 1 14\n",
            "5271\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  595 2 0 18\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  601 3 0 11\n",
            "5267\n",
            "337/337 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  599 0 0 16\n",
            "5263\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  599 2 0 14\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  603 1 0 11\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 2 2 15\n",
            "Acc_arr: mean=99.610 std=0.208\n",
            "bal_acc_arr: mean=97.697 std=2.988\n",
            "G_Mean_arr: mean=97.627 std=3.108\n",
            "F_Measure_arr: mean=92.226 std=4.043\n",
            "Precision_arr: mean=89.399 std=5.632\n",
            "Sensitivity_arr: mean=95.678 std=5.987\n",
            "Specificity_arr: mean=99.717 std=0.150\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  597 1 0 17\n",
            "5263\n",
            "338/338 [==============================] - 1s 3ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  600 1 1 13\n",
            "5269\n",
            "335/335 [==============================] - 1s 3ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 2 3 14\n",
            "5265\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 2 1 14\n",
            "5271\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  596 1 0 18\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  602 2 0 11\n",
            "5267\n",
            "337/337 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  598 1 0 16\n",
            "5263\n",
            "335/335 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  600 1 1 13\n",
            "5257\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  599 5 0 11\n",
            "5269\n",
            "336/336 [==============================] - 1s 2ms/step\n",
            "20/20 [==============================] - 0s 2ms/step\n",
            "TN, FP, FN, TP:  594 4 2 15\n",
            "Acc_arr: mean=99.545 std=0.289\n",
            "bal_acc_arr: mean=97.315 std=2.943\n",
            "G_Mean_arr: mean=97.240 std=3.057\n",
            "F_Measure_arr: mean=90.878 std=5.549\n",
            "Precision_arr: mean=87.633 std=7.966\n",
            "Sensitivity_arr: mean=94.964 std=5.854\n",
            "Specificity_arr: mean=99.667 std=0.223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gD9L6fY0HVin"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}